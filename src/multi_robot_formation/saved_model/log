/usr/bin/python3.8 /home/xinchi/catkin_ws/src/multi_robot_formation/src/multi_robot_formation/model/vit_model.py
['/home/xinchi/catkin_ws/src/multi_robot_formation/src/multi_robot_formation/model', '/home/xinchi/catkin_ws/src/multi_robot_formation', '/snap/pycharm-professional/327/plugins/python/helpers/pycharm_display', '/usr/lib/python38.zip', '/usr/lib/python3.8', '/usr/lib/python3.8/lib-dynload', '/home/xinchi/.local/lib/python3.8/site-packages', '/usr/local/lib/python3.8/dist-packages', '/usr/lib/python3/dist-packages', '/snap/pycharm-professional/327/plugins/python/helpers/pycharm_matplotlib_backend', '/home/xinchi/catkin_ws/src/multi_robot_formation/src', '/home/xinchi/catkin_ws/src/multi_robot_formation/src/multi_robot_formation', '/home/xinchi/catkin_ws/src/multi_robot_formation/src/multi_robot_formation/model']
/home/xinchi/catkin_ws/src/multi_robot_formation/src/multi_robot_formation/model
100%|██████████| 100/100 [00:00<00:00, 569.83it/s]
(5, 200100, 3)
(5, 2001, 3)
mse
rms
100%|██████████| 1/1 [00:00<00:00, 6512.89it/s]
RMSprop (
Parameter Group 0
    alpha: 0.99
    centered: False
    differentiable: False
    eps: 1e-08
    foreach: None
    lr: 0.01
    maximize: False
    momentum: 0
    weight_decay: 0
)
  8%|▊         | 999/12506 [08:08<1:31:39,  2.09it/s]Average training_data loss at iteration 1000: 0.0083941485095073
loss  671.531880760584
total  80000
  8%|▊         | 1000/12506 [08:32<23:45:56,  7.44s/it]Average evaluating_data loss at iteration 1000: 0.022482019051452286
loss_eval  224.82019051452286
total_eval  10000
 16%|█▌        | 1999/12506 [16:36<1:23:57,  2.09it/s]Average training_data loss at iteration 2000: 0.005916213256940129
loss  473.2970605552103
total  80000
Average evaluating_data loss at iteration 2000: 0.022138584247101606
loss_eval  221.38584247101608
total_eval  10000
 24%|██▍       | 2999/12506 [25:00<1:16:29,  2.07it/s]Average training_data loss at iteration 3000: 0.005533092037448332
loss  442.6473629958665
total  80000
Average evaluating_data loss at iteration 3000: 0.020489310538536788
loss_eval  204.89310538536787
total_eval  10000
 32%|███▏      | 3999/12506 [33:25<1:08:00,  2.09it/s]Average training_data loss at iteration 4000: 0.005393716889811301
loss  431.49735118490406
total  80000
Average evaluating_data loss at iteration 4000: 0.018623998175092523
loss_eval  186.23998175092524
total_eval  10000
 40%|███▉      | 4999/12506 [41:49<1:00:00,  2.08it/s]Average training_data loss at iteration 5000: 0.004876089246758366
loss  390.0871397406693
total  80000
Average evaluating_data loss at iteration 5000: 0.01607501547799398
loss_eval  160.7501547799398
total_eval  10000
 48%|████▊     | 5999/12506 [50:15<51:58,  2.09it/s]Average training_data loss at iteration 6000: 0.004357983809081725
loss  348.63870472653804
total  80000
Average evaluating_data loss at iteration 6000: 0.017270306500805195
loss_eval  172.70306500805196
total_eval  10000
 56%|█████▌    | 6999/12506 [58:42<44:01,  2.08it/s]Average training_data loss at iteration 7000: 0.004672311561629796
loss  373.7849249303837
total  80000
Average evaluating_data loss at iteration 7000: 0.015034375827098972
loss_eval  150.34375827098972
total_eval  10000
 64%|██████▍   | 7999/12506 [1:07:08<36:03,  2.08it/s]Average training_data loss at iteration 8000: 0.004758040098969369
loss  380.6432079175495
total  80000
Average evaluating_data loss at iteration 8000: 0.015394032440235853
loss_eval  153.94032440235853
total_eval  10000
 72%|███████▏  | 8999/12506 [1:15:35<27:55,  2.09it/s]Average training_data loss at iteration 9000: 0.004345781963778241
loss  347.6625571022593
total  80000
Average evaluating_data loss at iteration 9000: 0.014053269422429565
loss_eval  140.53269422429565
total_eval  10000
 80%|███████▉  | 9999/12506 [1:24:04<20:18,  2.06it/s]Average training_data loss at iteration 10000: 0.004358554541106382
loss  348.68436328851055
total  80000
Average evaluating_data loss at iteration 10000: 0.014493390068253478
loss_eval  144.93390068253478
total_eval  10000
 88%|████████▊ | 10999/12506 [1:32:30<12:02,  2.09it/s]Average training_data loss at iteration 11000: 0.004447202207049596
loss  355.7761765639677
total  80000
Average evaluating_data loss at iteration 11000: 0.01460832094091667
loss_eval  146.0832094091667
total_eval  10000
 96%|█████████▌| 11999/12506 [1:40:55<04:03,  2.08it/s]Average training_data loss at iteration 12000: 0.004223998654477952
loss  337.9198923582362
total  80000
 96%|█████████▌| 12000/12506 [1:41:19<1:03:06,  7.48s/it]Average evaluating_data loss at iteration 12000: 0.013204203109675216
loss_eval  132.04203109675217
total_eval  10000
100%|██████████| 12506/12506 [1:45:23<00:00,  1.98it/s]
  8%|▊         | 999/12506 [08:01<1:32:54,  2.06it/s]Average training_data loss at iteration 1000: 0.004156681693770454
loss  500.7970104654643
total  120480
Average evaluating_data loss at iteration 1000: 0.012838082658166465
loss_eval  128.38082658166465
total_eval  10000
 16%|█▌        | 1999/12506 [16:26<1:23:56,  2.09it/s]Average training_data loss at iteration 2000: 0.0041239577747607305
loss  329.91662198085845
total  80000
 16%|█▌        | 2000/12506 [16:50<21:43:13,  7.44s/it]Average evaluating_data loss at iteration 2000: 0.015094755137816258
loss_eval  150.9475513781626
total_eval  10000
 24%|██▍       | 2999/12506 [24:53<1:15:59,  2.08it/s]Average training_data loss at iteration 3000: 0.004090387830808489
loss  327.23102646467913
total  80000
 24%|██▍       | 3000/12506 [25:17<19:45:52,  7.48s/it]Average evaluating_data loss at iteration 3000: 0.013629432560654298
loss_eval  136.29432560654297
total_eval  10000
 32%|███▏      | 3999/12506 [33:19<1:07:56,  2.09it/s]Average training_data loss at iteration 4000: 0.0041137148785815305
loss  329.09719028652245
total  80000
 32%|███▏      | 4000/12506 [33:43<17:40:27,  7.48s/it]Average evaluating_data loss at iteration 4000: 0.01543339369449535
loss_eval  154.3339369449535
total_eval  10000
 40%|███▉      | 4999/12506 [41:44<59:57,  2.09it/s]Average training_data loss at iteration 5000: 0.00400478745684579
loss  320.38299654766314
total  80000
Average evaluating_data loss at iteration 5000: 0.014701886022504447
loss_eval  147.01886022504448
total_eval  10000
 48%|████▊     | 5999/12506 [50:10<51:56,  2.09it/s]Average training_data loss at iteration 6000: 0.004072280918222773
loss  325.7824734578218
total  80000
 48%|████▊     | 6000/12506 [50:33<13:31:28,  7.48s/it]Average evaluating_data loss at iteration 6000: 0.013154692198973996
loss_eval  131.54692198973996
total_eval  10000
 56%|█████▌    | 6999/12506 [58:36<44:02,  2.08it/s]Average training_data loss at iteration 7000: 0.0039093269048631266
loss  312.7461523890501
total  80000
 56%|█████▌    | 7000/12506 [59:00<11:24:00,  7.45s/it]Average evaluating_data loss at iteration 7000: 0.013546989636564083
loss_eval  135.46989636564084
total_eval  10000
 64%|██████▍   | 7999/12506 [1:07:01<36:01,  2.09it/s]Average training_data loss at iteration 8000: 0.004072166668489758
loss  325.7733334791806
total  80000
 64%|██████▍   | 8000/12506 [1:07:25<9:20:10,  7.46s/it]Average evaluating_data loss at iteration 8000: 0.012197094648935234
loss_eval  121.97094648935234
total_eval  10000
 72%|███████▏  | 8999/12506 [1:15:26<28:01,  2.09it/s]Average training_data loss at iteration 9000: 0.003941161605157873
loss  315.29292841262986
total  80000
Average evaluating_data loss at iteration 9000: 0.012637701826286553
loss_eval  126.37701826286553
total_eval  10000
 80%|███████▉  | 9999/12506 [1:23:50<20:05,  2.08it/s]Average training_data loss at iteration 10000: 0.00389353023899503
loss  311.4824191196024
total  80000
 80%|███████▉  | 10000/12506 [1:24:13<5:11:45,  7.46s/it]Average evaluating_data loss at iteration 10000: 0.013912816836918364
loss_eval  139.12816836918364
total_eval  10000
 88%|████████▊ | 10999/12506 [1:32:13<12:00,  2.09it/s]Average training_data loss at iteration 11000: 0.003966139654065217
loss  317.2911723252174
total  80000
Average evaluating_data loss at iteration 11000: 0.013750380546679285
loss_eval  137.50380546679284
total_eval  10000
 96%|█████████▌| 11999/12506 [1:40:37<04:02,  2.09it/s]Average training_data loss at iteration 12000: 0.003823504561894635
loss  305.8803649515708
total  80000
 96%|█████████▌| 12000/12506 [1:41:01<1:02:43,  7.44s/it]Average evaluating_data loss at iteration 12000: 0.012759123216593579
loss_eval  127.59123216593579
total_eval  10000
100%|██████████| 12506/12506 [1:45:04<00:00,  1.98it/s]
  8%|▊         | 999/12506 [07:59<1:32:02,  2.08it/s]Average training_data loss at iteration 1000: 0.0038558455323811805
loss  464.55226974128465
total  120480
  8%|▊         | 1000/12506 [08:23<23:45:54,  7.44s/it]Average evaluating_data loss at iteration 1000: 0.01212332883315114
loss_eval  121.23328833151139
total_eval  10000
 16%|█▌        | 1999/12506 [16:23<1:23:56,  2.09it/s]Average training_data loss at iteration 2000: 0.003747886877945719
loss  299.8309502356575
total  80000
Average evaluating_data loss at iteration 2000: 0.012318241702442537
loss_eval  123.18241702442536
total_eval  10000
 24%|██▍       | 2999/12506 [24:46<1:16:50,  2.06it/s]Average training_data loss at iteration 3000: 0.0036416774133128703
loss  291.33419306502964
total  80000
 24%|██▍       | 3000/12506 [25:10<19:40:25,  7.45s/it]Average evaluating_data loss at iteration 3000: 0.013864181661496073
loss_eval  138.64181661496073
total_eval  10000
 32%|███▏      | 3999/12506 [33:10<1:07:52,  2.09it/s]Average training_data loss at iteration 4000: 0.003919841050776432
loss  313.5872840621146
total  80000
Average evaluating_data loss at iteration 4000: 0.012896961705528657
loss_eval  128.96961705528656
total_eval  10000
 40%|███▉      | 4999/12506 [41:33<59:53,  2.09it/s]Average training_data loss at iteration 5000: 0.0037769752422041926
loss  302.1580193763354
total  80000
Average evaluating_data loss at iteration 5000: 0.013462898707283764
loss_eval  134.62898707283765
total_eval  10000
 48%|████▊     | 5999/12506 [49:57<52:05,  2.08it/s]Average training_data loss at iteration 6000: 0.00394391682508778
loss  315.5133460070224
total  80000
 48%|████▊     | 6000/12506 [50:20<13:27:54,  7.45s/it]Average evaluating_data loss at iteration 6000: 0.01313143879939521
loss_eval  131.3143879939521
total_eval  10000
 56%|█████▌    | 6999/12506 [58:20<43:56,  2.09it/s]Average training_data loss at iteration 7000: 0.0036761425621774442
loss  294.09140497419554
total  80000
Average evaluating_data loss at iteration 7000: 0.011884029536732406
loss_eval  118.84029536732406
total_eval  10000
 64%|██████▍   | 7999/12506 [1:06:43<36:00,  2.09it/s]Average training_data loss at iteration 8000: 0.00365852068874366
loss  292.6816550994928
total  80000
 64%|██████▍   | 8000/12506 [1:07:07<9:18:32,  7.44s/it]Average evaluating_data loss at iteration 8000: 0.012943976125864402
loss_eval  129.439761258644
total_eval  10000
 72%|███████▏  | 8999/12506 [1:15:06<27:59,  2.09it/s]Average training_data loss at iteration 9000: 0.0038127606384817632
loss  305.02085107854106
total  80000
Average evaluating_data loss at iteration 9000: 0.013905144104228526
loss_eval  139.05144104228526
total_eval  10000
 80%|███████▉  | 9999/12506 [1:23:29<20:03,  2.08it/s]Average training_data loss at iteration 10000: 0.003508253845104405
loss  280.6603076083524
total  80000
Average evaluating_data loss at iteration 10000: 0.013019708490840927
loss_eval  130.19708490840927
total_eval  10000
 88%|████████▊ | 10999/12506 [1:31:53<12:01,  2.09it/s]Average training_data loss at iteration 11000: 0.003910730813943868
loss  312.8584651155095
total  80000
Average evaluating_data loss at iteration 11000: 0.012512126834949249
loss_eval  125.12126834949248
total_eval  10000
 96%|█████████▌| 11999/12506 [1:40:16<04:02,  2.09it/s]Average training_data loss at iteration 12000: 0.0035555672413847468
loss  284.44537931077974
total  80000
Average evaluating_data loss at iteration 12000: 0.012366351290492269
loss_eval  123.66351290492268
total_eval  10000
100%|██████████| 12506/12506 [1:44:42<00:00,  1.99it/s]
  8%|▊         | 999/12506 [07:59<1:31:59,  2.08it/s]Average training_data loss at iteration 1000: 0.003533271710720587
loss  425.68857570761634
total  120480
  8%|▊         | 1000/12506 [08:22<23:50:55,  7.46s/it]Average evaluating_data loss at iteration 1000: 0.011881777319761758
loss_eval  118.81777319761758
total_eval  10000
 16%|█▌        | 1999/12506 [16:22<1:23:49,  2.09it/s]Average training_data loss at iteration 2000: 0.0037685861697185786
loss  301.4868935774863
total  80000
 16%|█▌        | 2000/12506 [16:45<21:41:42,  7.43s/it]Average evaluating_data loss at iteration 2000: 0.012062604547182108
loss_eval  120.62604547182109
total_eval  10000
 24%|██▍       | 2999/12506 [24:44<1:15:44,  2.09it/s]Average training_data loss at iteration 3000: 0.00362425880862601
loss  289.9407046900808
total  80000
Average evaluating_data loss at iteration 3000: 0.013408741352878204
loss_eval  134.08741352878204
total_eval  10000
 32%|███▏      | 3999/12506 [33:07<1:07:55,  2.09it/s]Average training_data loss at iteration 4000: 0.003524491198275784
loss  281.95929586206273
total  80000
Average evaluating_data loss at iteration 4000: 0.01330452771096183
loss_eval  133.0452771096183
total_eval  10000
 40%|███▉      | 4999/12506 [41:30<59:54,  2.09it/s]Average training_data loss at iteration 5000: 0.0037281376724533663
loss  298.2510137962693
total  80000
 40%|███▉      | 5000/12506 [41:53<15:30:34,  7.44s/it]Average evaluating_data loss at iteration 5000: 0.012549456755720664
loss_eval  125.49456755720664
total_eval  10000
 48%|████▊     | 5999/12506 [49:52<51:59,  2.09it/s]Average training_data loss at iteration 6000: 0.0035984210312920155
loss  287.87368250336124
total  80000
Average evaluating_data loss at iteration 6000: 0.012333567540033162
loss_eval  123.33567540033162
total_eval  10000
 56%|█████▌    | 6999/12506 [58:16<43:56,  2.09it/s]Average training_data loss at iteration 7000: 0.0034631897545280865
loss  277.0551803622469
total  80000
 56%|█████▌    | 7000/12506 [58:39<11:22:33,  7.44s/it]Average evaluating_data loss at iteration 7000: 0.012575834496458375
loss_eval  125.75834496458376
total_eval  10000
 64%|██████▍   | 7999/12506 [1:06:39<35:58,  2.09it/s]Average training_data loss at iteration 8000: 0.0035545162150982263
loss  284.3612972078581
total  80000
 64%|██████▍   | 8000/12506 [1:07:02<9:18:39,  7.44s/it]Average evaluating_data loss at iteration 8000: 0.011394798896807348
loss_eval  113.94798896807349
total_eval  10000
 72%|███████▏  | 8999/12506 [1:15:02<28:01,  2.09it/s]Average training_data loss at iteration 9000: 0.0036367295470845916
loss  290.93836376676734
total  80000
 72%|███████▏  | 9000/12506 [1:15:25<7:15:11,  7.45s/it]Average evaluating_data loss at iteration 9000: 0.011888627871642545
loss_eval  118.88627871642545
total_eval  10000
 80%|███████▉  | 9999/12506 [1:23:25<20:11,  2.07it/s]Average training_data loss at iteration 10000: 0.0036653397304059355
loss  293.22717843247483
total  80000
 80%|███████▉  | 10000/12506 [1:23:49<5:10:42,  7.44s/it]Average evaluating_data loss at iteration 10000: 0.01064781973619423
loss_eval  106.47819736194229
total_eval  10000
 88%|████████▊ | 10999/12506 [1:31:48<12:01,  2.09it/s]Average training_data loss at iteration 11000: 0.003701874135843962
loss  296.14993086751696
total  80000
Average evaluating_data loss at iteration 11000: 0.011437315772935918
loss_eval  114.37315772935918
total_eval  10000
 96%|█████████▌| 11999/12506 [1:40:11<04:02,  2.09it/s]Average training_data loss at iteration 12000: 0.0035725982959098077
loss  285.8078636727846
total  80000
Average evaluating_data loss at iteration 12000: 0.011419736885114643
loss_eval  114.19736885114642
total_eval  10000
100%|██████████| 12506/12506 [1:44:38<00:00,  1.99it/s]
  8%|▊         | 999/12506 [07:59<1:32:09,  2.08it/s]Average training_data loss at iteration 1000: 0.0035065787776042464
loss  422.4726111257596
total  120480
  8%|▊         | 1000/12506 [08:23<23:46:18,  7.44s/it]Average evaluating_data loss at iteration 1000: 0.013370527208058737
loss_eval  133.70527208058738
total_eval  10000
 16%|█▌        | 1999/12506 [16:22<1:23:54,  2.09it/s]Average training_data loss at iteration 2000: 0.0035731787389263374
loss  285.854299114107
total  80000
Average evaluating_data loss at iteration 2000: 0.013022179721842648
loss_eval  130.22179721842647
total_eval  10000
 24%|██▍       | 2999/12506 [24:45<1:15:50,  2.09it/s]Average training_data loss at iteration 3000: 0.0034675943488421323
loss  277.4075479073706
total  80000
Average evaluating_data loss at iteration 3000: 0.012751747140232089
loss_eval  127.5174714023209
total_eval  10000
 32%|███▏      | 3999/12506 [33:08<1:07:59,  2.09it/s]Average training_data loss at iteration 4000: 0.0036967995131908823
loss  295.7439610552706
total  80000
 32%|███▏      | 4000/12506 [33:32<17:40:03,  7.48s/it]Average evaluating_data loss at iteration 4000: 0.012772841147262994
loss_eval  127.72841147262994
total_eval  10000
 40%|███▉      | 4999/12506 [41:31<59:52,  2.09it/s]Average training_data loss at iteration 5000: 0.00365902508784007
loss  292.7220070272056
total  80000
Average evaluating_data loss at iteration 5000: 0.01297140172693429
loss_eval  129.7140172693429
total_eval  10000
 48%|████▊     | 5999/12506 [49:53<51:59,  2.09it/s]Average training_data loss at iteration 6000: 0.003645981674059777
loss  291.6785339247822
total  80000
 48%|████▊     | 6000/12506 [50:17<13:29:33,  7.47s/it]Average evaluating_data loss at iteration 6000: 0.013521926833120534
loss_eval  135.21926833120534
total_eval  10000
 56%|█████▌    | 6999/12506 [58:17<43:57,  2.09it/s]Average training_data loss at iteration 7000: 0.003746300198632784
loss  299.7040158906227
total  80000
 56%|█████▌    | 7000/12506 [58:40<11:22:24,  7.44s/it]Average evaluating_data loss at iteration 7000: 0.01167874251833483
loss_eval  116.7874251833483
total_eval  10000
 64%|██████▍   | 7999/12506 [1:06:40<35:59,  2.09it/s]Average training_data loss at iteration 8000: 0.003448576244802507
loss  275.88609958420057
total  80000
 64%|██████▍   | 8000/12506 [1:07:03<9:19:17,  7.45s/it]Average evaluating_data loss at iteration 8000: 0.011364908966031002
loss_eval  113.64908966031001
total_eval  10000
 72%|███████▏  | 8999/12506 [1:15:03<28:02,  2.08it/s]Average training_data loss at iteration 9000: 0.0034312009929522553
loss  274.4960794361804
total  80000
 72%|███████▏  | 9000/12506 [1:15:27<7:14:52,  7.44s/it]Average evaluating_data loss at iteration 9000: 0.01211909701062115
loss_eval  121.1909701062115
total_eval  10000
 80%|███████▉  | 9999/12506 [1:23:26<19:59,  2.09it/s]Average training_data loss at iteration 10000: 0.0034134205406403254
loss  273.07364325122603
total  80000
 80%|███████▉  | 10000/12506 [1:23:50<5:10:36,  7.44s/it]Average evaluating_data loss at iteration 10000: 0.0112903304184042
loss_eval  112.903304184042
total_eval  10000
 88%|████████▊ | 10999/12506 [1:31:50<12:01,  2.09it/s]Average training_data loss at iteration 11000: 0.0034975048300266757
loss  279.80038640213405
total  80000
Average evaluating_data loss at iteration 11000: 0.010437091193360103
loss_eval  104.37091193360104
total_eval  10000
 96%|█████████▌| 11999/12506 [1:40:14<04:03,  2.09it/s]Average training_data loss at iteration 12000: 0.0036355923751562698
loss  290.8473900125016
total  80000
 96%|█████████▌| 12000/12506 [1:40:37<1:02:42,  7.44s/it]Average evaluating_data loss at iteration 12000: 0.011063602407330482
loss_eval  110.63602407330482
total_eval  10000
100%|██████████| 12506/12506 [1:44:40<00:00,  1.99it/s]
  8%|▊         | 999/12506 [08:00<1:32:14,  2.08it/s]Average training_data loss at iteration 1000: 0.003619058815974811
loss  436.0242061486452
total  120480
  8%|▊         | 1000/12506 [08:23<23:51:45,  7.47s/it]Average evaluating_data loss at iteration 1000: 0.012682641499667819
loss_eval  126.8264149966782
total_eval  10000
 16%|█▌        | 1999/12506 [16:24<1:23:47,  2.09it/s]Average training_data loss at iteration 2000: 0.0035821129461408668
loss  286.56903569126933
total  80000
 16%|█▌        | 2000/12506 [16:47<21:47:45,  7.47s/it]Average evaluating_data loss at iteration 2000: 0.010414014677433516
loss_eval  104.14014677433516
total_eval  10000
 24%|██▍       | 2999/12506 [24:48<1:15:51,  2.09it/s]Average training_data loss at iteration 3000: 0.003398471876977524
loss  271.87775015820193
total  80000
Average evaluating_data loss at iteration 3000: 0.0114317242807154
loss_eval  114.317242807154
total_eval  10000
 32%|███▏      | 3999/12506 [33:11<1:07:54,  2.09it/s]Average training_data loss at iteration 4000: 0.003450675506530078
loss  276.05404052240624
total  80000
 32%|███▏      | 4000/12506 [33:35<17:37:40,  7.46s/it]Average evaluating_data loss at iteration 4000: 0.011542067185928154
loss_eval  115.42067185928154
total_eval  10000
 40%|███▉      | 4999/12506 [41:35<59:54,  2.09it/s]Average training_data loss at iteration 5000: 0.0035314529735342953
loss  282.5162378827436
total  80000
Average evaluating_data loss at iteration 5000: 0.011139293805264317
loss_eval  111.39293805264317
total_eval  10000
 48%|████▊     | 5999/12506 [49:59<51:54,  2.09it/s]Average training_data loss at iteration 6000: 0.003501327598221102
loss  280.10620785768816
total  80000
 48%|████▊     | 6000/12506 [50:23<13:26:32,  7.44s/it]Average evaluating_data loss at iteration 6000: 0.010438383531875007
loss_eval  104.38383531875007
total_eval  10000
 56%|█████▌    | 6999/12506 [58:22<44:01,  2.08it/s]Average training_data loss at iteration 7000: 0.0032918836850595163
loss  263.3506948047613
total  80000
 56%|█████▌    | 7000/12506 [58:46<11:22:53,  7.44s/it]Average evaluating_data loss at iteration 7000: 0.011993655846663339
loss_eval  119.93655846663339
total_eval  10000
 64%|██████▍   | 7999/12506 [1:06:46<35:57,  2.09it/s]Average training_data loss at iteration 8000: 0.003441581926133666
loss  275.32655409069326
total  80000
 64%|██████▍   | 8000/12506 [1:07:09<9:18:23,  7.44s/it]Average evaluating_data loss at iteration 8000: 0.010900686513811195
loss_eval  109.00686513811195
total_eval  10000
 72%|███████▏  | 8999/12506 [1:15:09<27:57,  2.09it/s]Average training_data loss at iteration 9000: 0.0033327513874414952
loss  266.62011099531964
total  80000
Average evaluating_data loss at iteration 9000: 0.010621728608745743
loss_eval  106.21728608745744
total_eval  10000
 80%|███████▉  | 9999/12506 [1:23:32<20:00,  2.09it/s]Average training_data loss at iteration 10000: 0.003337957818977273
loss  267.03662551818184
total  80000
 80%|███████▉  | 10000/12506 [1:23:56<5:10:51,  7.44s/it]Average evaluating_data loss at iteration 10000: 0.011009662196714799
loss_eval  110.096621967148
total_eval  10000
 88%|████████▊ | 10999/12506 [1:31:55<12:02,  2.09it/s]Average training_data loss at iteration 11000: 0.00346574633571024
loss  277.2597068568192
total  80000
Average evaluating_data loss at iteration 11000: 0.01234858291359862
loss_eval  123.48582913598621
total_eval  10000
 96%|█████████▌| 11999/12506 [1:40:18<04:02,  2.09it/s]Average training_data loss at iteration 12000: 0.0033845341732312057
loss  270.76273385849646
total  80000
Average evaluating_data loss at iteration 12000: 0.010918749228246288
loss_eval  109.18749228246288
total_eval  10000
100%|██████████| 12506/12506 [1:44:45<00:00,  1.99it/s]
  8%|▊         | 999/12506 [07:59<1:31:55,  2.09it/s]Average training_data loss at iteration 1000: 0.003455275224747048
loss  416.29155907752437
total  120480
Average evaluating_data loss at iteration 1000: 0.010484011734395754
loss_eval  104.84011734395754
total_eval  10000
 16%|█▌        | 1999/12506 [16:22<1:23:56,  2.09it/s]Average training_data loss at iteration 2000: 0.0034239062262607077
loss  273.9124981008566
total  80000
Average evaluating_data loss at iteration 2000: 0.01274536520499203
loss_eval  127.45365204992031
total_eval  10000
 24%|██▍       | 2999/12506 [24:45<1:15:59,  2.09it/s]Average training_data loss at iteration 3000: 0.0034106196298963752
loss  272.84957039171
total  80000
 24%|██▍       | 3000/12506 [25:08<19:40:08,  7.45s/it]Average evaluating_data loss at iteration 3000: 0.01199404165276317
loss_eval  119.94041652763171
total_eval  10000
 32%|███▏      | 3999/12506 [33:08<1:07:56,  2.09it/s]Average training_data loss at iteration 4000: 0.0034703041785765294
loss  277.62433428612235
total  80000
Average evaluating_data loss at iteration 4000: 0.012016844688542389
loss_eval  120.16844688542389
total_eval  10000
 40%|███▉      | 4999/12506 [41:32<1:00:05,  2.08it/s]Average training_data loss at iteration 5000: 0.0036175314449936606
loss  289.40251559949286
total  80000
 40%|███▉      | 5000/12506 [41:55<15:30:42,  7.44s/it]Average evaluating_data loss at iteration 5000: 0.01059992895098043
loss_eval  105.9992895098043
total_eval  10000
 48%|████▊     | 5999/12506 [49:55<51:55,  2.09it/s]Average training_data loss at iteration 6000: 0.0034387162441328876
loss  275.097299530631
total  80000
 48%|████▊     | 6000/12506 [50:19<13:28:30,  7.46s/it]Average evaluating_data loss at iteration 6000: 0.010983454269419995
loss_eval  109.83454269419995
total_eval  10000
 56%|█████▌    | 6999/12506 [58:19<44:00,  2.09it/s]Average training_data loss at iteration 7000: 0.003312080669692363
loss  264.96645357538904
total  80000
 56%|█████▌    | 7000/12506 [58:43<11:22:33,  7.44s/it]Average evaluating_data loss at iteration 7000: 0.012624977960335838
loss_eval  126.24977960335838
total_eval  10000
 64%|██████▍   | 7999/12506 [1:06:43<36:01,  2.08it/s]Average training_data loss at iteration 8000: 0.0034807264154506547
loss  278.4581132360524
total  80000
Average evaluating_data loss at iteration 8000: 0.011196195976867444
loss_eval  111.96195976867443
total_eval  10000
 72%|███████▏  | 8999/12506 [1:15:07<27:57,  2.09it/s]Average training_data loss at iteration 9000: 0.003317393232952426
loss  265.3914586361941
total  80000
 72%|███████▏  | 9000/12506 [1:15:30<7:15:14,  7.45s/it]Average evaluating_data loss at iteration 9000: 0.012062021834450641
loss_eval  120.62021834450641
total_eval  10000
 80%|███████▉  | 9999/12506 [1:23:31<20:00,  2.09it/s]Average training_data loss at iteration 10000: 0.003353703450714702
loss  268.29627605717616
total  80000
 80%|███████▉  | 10000/12506 [1:23:54<5:10:35,  7.44s/it]Average evaluating_data loss at iteration 10000: 0.011156331610695264
loss_eval  111.56331610695264
total_eval  10000
 88%|████████▊ | 10999/12506 [1:31:54<12:02,  2.09it/s]Average training_data loss at iteration 11000: 0.0036740218594608193
loss  293.92174875686555
total  80000
 88%|████████▊ | 11000/12506 [1:32:18<3:06:44,  7.44s/it]Average evaluating_data loss at iteration 11000: 0.01074525113884781
loss_eval  107.4525113884781
total_eval  10000
 96%|█████████▌| 11999/12506 [1:40:17<04:03,  2.08it/s]Average training_data loss at iteration 12000: 0.003466538363369961
loss  277.3230690695969
total  80000
Average evaluating_data loss at iteration 12000: 0.011518593270086336
loss_eval  115.18593270086336
total_eval  10000
100%|██████████| 12506/12506 [1:44:44<00:00,  1.99it/s]
  8%|▊         | 999/12506 [07:59<1:31:41,  2.09it/s]Average training_data loss at iteration 1000: 0.0034370797767549984
loss  414.0993715034422
total  120480
Average evaluating_data loss at iteration 1000: 0.010661096013945158
loss_eval  106.61096013945158
total_eval  10000
 16%|█▌        | 1999/12506 [16:23<1:23:56,  2.09it/s]Average training_data loss at iteration 2000: 0.0032309088953997433
loss  258.47271163197945
total  80000
 16%|█▌        | 2000/12506 [16:46<21:48:27,  7.47s/it]Average evaluating_data loss at iteration 2000: 0.010340866512079596
loss_eval  103.40866512079596
total_eval  10000
 24%|██▍       | 2999/12506 [24:46<1:15:57,  2.09it/s]Average training_data loss at iteration 3000: 0.0034972898716565497
loss  279.78318973252397
total  80000
Average evaluating_data loss at iteration 3000: 0.011348280624375706
loss_eval  113.48280624375705
total_eval  10000
 32%|███▏      | 3999/12506 [33:09<1:07:48,  2.09it/s]Average training_data loss at iteration 4000: 0.0032740859337011124
loss  261.926874696089
total  80000
 32%|███▏      | 4000/12506 [33:33<17:37:49,  7.46s/it]Average evaluating_data loss at iteration 4000: 0.010984199127999065
loss_eval  109.84199127999064
total_eval  10000
 40%|███▉      | 4999/12506 [41:32<59:52,  2.09it/s]Average training_data loss at iteration 5000: 0.003455452433053169
loss  276.43619464425353
total  80000
 40%|███▉      | 5000/12506 [41:56<15:34:31,  7.47s/it]Average evaluating_data loss at iteration 5000: 0.011269875768780847
loss_eval  112.69875768780847
total_eval  10000
 48%|████▊     | 5999/12506 [49:55<51:55,  2.09it/s]Average training_data loss at iteration 6000: 0.003295676024692504
loss  263.65408197540035
total  80000
 48%|████▊     | 6000/12506 [50:19<13:26:07,  7.43s/it]Average evaluating_data loss at iteration 6000: 0.010568278208429597
loss_eval  105.68278208429597
total_eval  10000
 56%|█████▌    | 6999/12506 [58:18<43:56,  2.09it/s]Average training_data loss at iteration 7000: 0.003415599284485311
loss  273.2479427588249
total  80000
 56%|█████▌    | 7000/12506 [58:42<11:22:34,  7.44s/it]Average evaluating_data loss at iteration 7000: 0.010159053162033823
loss_eval  101.59053162033824
total_eval  10000
 64%|██████▍   | 7999/12506 [1:06:41<36:24,  2.06it/s]Average training_data loss at iteration 8000: 0.003319988535293972
loss  265.5990828235178
total  80000
 64%|██████▍   | 8000/12506 [1:07:05<9:18:47,  7.44s/it]Average evaluating_data loss at iteration 8000: 0.010965348972650724
loss_eval  109.65348972650723
total_eval  10000
 72%|███████▏  | 8999/12506 [1:15:04<27:58,  2.09it/s]Average training_data loss at iteration 9000: 0.003323144461417804
loss  265.8515569134243
total  80000
 72%|███████▏  | 9000/12506 [1:15:28<7:14:30,  7.44s/it]Average evaluating_data loss at iteration 9000: 0.011287774751243084
loss_eval  112.87774751243084
total_eval  10000
 80%|███████▉  | 9999/12506 [1:23:27<20:00,  2.09it/s]Average training_data loss at iteration 10000: 0.003396643764878592
loss  271.73150119028736
total  80000
Average evaluating_data loss at iteration 10000: 0.010731997090446563
loss_eval  107.31997090446563
total_eval  10000
 88%|████████▊ | 10999/12506 [1:31:51<12:38,  1.99it/s]Average training_data loss at iteration 11000: 0.0033335892421502656
loss  266.68713937202125
total  80000
 88%|████████▊ | 11000/12506 [1:32:15<3:07:14,  7.46s/it]Average evaluating_data loss at iteration 11000: 0.012011836032863665
loss_eval  120.11836032863664
total_eval  10000
 96%|█████████▌| 11999/12506 [1:40:14<04:03,  2.08it/s]Average training_data loss at iteration 12000: 0.0032420551825764845
loss  259.36441460611877
total  80000
Average evaluating_data loss at iteration 12000: 0.011675766899277374
loss_eval  116.75766899277374
total_eval  10000
100%|██████████| 12506/12506 [1:44:41<00:00,  1.99it/s]
  8%|▊         | 999/12506 [07:59<1:31:51,  2.09it/s]Average training_data loss at iteration 1000: 0.0032809062167878573
loss  395.28358099860105
total  120480
Average evaluating_data loss at iteration 1000: 0.012334112615853775
loss_eval  123.34112615853775
total_eval  10000
 16%|█▌        | 1999/12506 [16:22<1:23:53,  2.09it/s]Average training_data loss at iteration 2000: 0.0033418533790996073
loss  267.3482703279686
total  80000
 16%|█▌        | 2000/12506 [16:46<21:42:08,  7.44s/it]Average evaluating_data loss at iteration 2000: 0.011225331980381297
loss_eval  112.25331980381297
total_eval  10000
 24%|██▍       | 2999/12506 [24:45<1:15:55,  2.09it/s]Average training_data loss at iteration 3000: 0.003478487173243521
loss  278.27897385948165
total  80000
Average evaluating_data loss at iteration 3000: 0.011481284258649542
loss_eval  114.81284258649542
total_eval  10000
 32%|███▏      | 3999/12506 [33:08<1:08:01,  2.08it/s]Average training_data loss at iteration 4000: 0.0032661640595866574
loss  261.2931247669326
total  80000
 32%|███▏      | 4000/12506 [33:32<17:35:59,  7.45s/it]Average evaluating_data loss at iteration 4000: 0.01094910463074641
loss_eval  109.49104630746409
total_eval  10000
 40%|███▉      | 4999/12506 [41:31<1:01:24,  2.04it/s]Average training_data loss at iteration 5000: 0.00329753377954249
loss  263.8027023633992
total  80000
Average evaluating_data loss at iteration 5000: 0.01095498019110816
loss_eval  109.5498019110816
total_eval  10000
 48%|████▊     | 5999/12506 [49:54<51:56,  2.09it/s]Average training_data loss at iteration 6000: 0.003336530924598742
loss  266.92247396789935
total  80000
Average evaluating_data loss at iteration 6000: 0.012293336720920924
loss_eval  122.93336720920924
total_eval  10000
 56%|█████▌    | 6999/12506 [58:17<44:20,  2.07it/s]Average training_data loss at iteration 7000: 0.0031034641705736626
loss  248.277133645893
total  80000
 56%|█████▌    | 7000/12506 [58:41<11:22:47,  7.44s/it]Average evaluating_data loss at iteration 7000: 0.012614366371446238
loss_eval  126.14366371446239
total_eval  10000
 64%|██████▍   | 7999/12506 [1:06:40<35:56,  2.09it/s]Average training_data loss at iteration 8000: 0.0034347916356150116
loss  274.7833308492009
total  80000
 64%|██████▍   | 8000/12506 [1:07:04<9:18:25,  7.44s/it]Average evaluating_data loss at iteration 8000: 0.0109809451210396
loss_eval  109.809451210396
total_eval  10000
 72%|███████▏  | 8999/12506 [1:15:03<27:58,  2.09it/s]Average training_data loss at iteration 9000: 0.003226761305630898
loss  258.14090445047185
total  80000
Average evaluating_data loss at iteration 9000: 0.01186506592299954
loss_eval  118.65065922999541
total_eval  10000
 80%|███████▉  | 9999/12506 [1:23:25<20:00,  2.09it/s]Average training_data loss at iteration 10000: 0.0032734673919131993
loss  261.87739135305594
total  80000
Average evaluating_data loss at iteration 10000: 0.013146588634236815
loss_eval  131.46588634236815
total_eval  10000
 88%|████████▊ | 10999/12506 [1:31:48<12:07,  2.07it/s]Average training_data loss at iteration 11000: 0.0031667112722302956
loss  253.33690177842365
total  80000
Average evaluating_data loss at iteration 11000: 0.012276434598380523
loss_eval  122.76434598380523
total_eval  10000
 96%|█████████▌| 11999/12506 [1:40:11<04:04,  2.08it/s]Average training_data loss at iteration 12000: 0.003279120104416634
loss  262.32960835333074
total  80000
 96%|█████████▌| 12000/12506 [1:40:34<1:02:41,  7.43s/it]Average evaluating_data loss at iteration 12000: 0.009999998746592572
loss_eval  99.99998746592573
total_eval  10000
100%|██████████| 12506/12506 [1:44:37<00:00,  1.99it/s]
  8%|▊         | 999/12506 [07:58<1:32:16,  2.08it/s]Average training_data loss at iteration 1000: 0.003239558994940349
loss  390.3020677104132
total  120480
  8%|▊         | 1000/12506 [08:22<23:46:30,  7.44s/it]Average evaluating_data loss at iteration 1000: 0.013132717358608142
loss_eval  131.32717358608141
total_eval  10000
 16%|█▌        | 1999/12506 [16:21<1:23:52,  2.09it/s]Average training_data loss at iteration 2000: 0.0032820692102638626
loss  262.565536821109
total  80000
Average evaluating_data loss at iteration 2000: 0.010996061094108488
loss_eval  109.96061094108488
total_eval  10000
 24%|██▍       | 2999/12506 [24:44<1:18:52,  2.01it/s]Average training_data loss at iteration 3000: 0.0033382866927699523
loss  267.0629354215962
total  80000
 24%|██▍       | 3000/12506 [25:08<19:45:42,  7.48s/it]Average evaluating_data loss at iteration 3000: 0.011243167117734487
loss_eval  112.43167117734487
total_eval  10000
 32%|███▏      | 3999/12506 [33:07<1:07:51,  2.09it/s]Average training_data loss at iteration 4000: 0.00349020129149501
loss  279.2161033196008
total  80000
 32%|███▏      | 4000/12506 [33:31<17:34:37,  7.44s/it]Average evaluating_data loss at iteration 4000: 0.011561413066879597
loss_eval  115.61413066879597
total_eval  10000
 40%|███▉      | 4999/12506 [41:30<59:58,  2.09it/s]  Average training_data loss at iteration 5000: 0.003175811781587359
loss  254.06494252698872
total  80000
 40%|███▉      | 5000/12506 [41:54<15:30:20,  7.44s/it]Average evaluating_data loss at iteration 5000: 0.01104935224183572
loss_eval  110.4935224183572
total_eval  10000
 48%|████▊     | 5999/12506 [49:54<51:57,  2.09it/s]Average training_data loss at iteration 6000: 0.003378327577783004
loss  270.26620622264033
total  80000
 48%|████▊     | 6000/12506 [50:17<13:26:13,  7.44s/it]Average evaluating_data loss at iteration 6000: 0.012069782020274993
loss_eval  120.69782020274992
total_eval  10000
 56%|█████▌    | 6999/12506 [58:17<44:01,  2.08it/s]Average training_data loss at iteration 7000: 0.003149103106750665
loss  251.9282485400532
total  80000
 56%|█████▌    | 7000/12506 [58:41<11:22:26,  7.44s/it]Average evaluating_data loss at iteration 7000: 0.010376392356406739
loss_eval  103.7639235640674
total_eval  10000
 64%|██████▍   | 7999/12506 [1:06:40<36:00,  2.09it/s]Average training_data loss at iteration 8000: 0.003137543560114831
loss  251.00348480918646
total  80000
 64%|██████▍   | 8000/12506 [1:07:04<9:18:20,  7.43s/it]Average evaluating_data loss at iteration 8000: 0.011954710692094835
loss_eval  119.54710692094835
total_eval  10000
 72%|███████▏  | 8999/12506 [1:15:03<27:59,  2.09it/s]Average training_data loss at iteration 9000: 0.0032260651548608626
loss  258.085212388869
total  80000
Average evaluating_data loss at iteration 9000: 0.010869685265846358
loss_eval  108.69685265846357
total_eval  10000
 80%|███████▉  | 9999/12506 [1:23:27<21:50,  1.91it/s]Average training_data loss at iteration 10000: 0.0032436766812077708
loss  259.49413449662165
total  80000
 80%|███████▉  | 10000/12506 [1:23:51<5:12:33,  7.48s/it]Average evaluating_data loss at iteration 10000: 0.011758819062874864
loss_eval  117.58819062874863
total_eval  10000
 88%|████████▊ | 10999/12506 [1:31:50<12:01,  2.09it/s]Average training_data loss at iteration 11000: 0.003384788186395967
loss  270.78305491167737
total  80000
Average evaluating_data loss at iteration 11000: 0.009918537155199353
loss_eval  99.18537155199353
total_eval  10000
 96%|█████████▌| 11999/12506 [1:40:14<04:02,  2.09it/s]Average training_data loss at iteration 12000: 0.003202450809006651
loss  256.1960647205321
total  80000
Average evaluating_data loss at iteration 12000: 0.011438156986345432
loss_eval  114.38156986345432
total_eval  10000
100%|██████████| 12506/12506 [1:44:40<00:00,  1.99it/s]

Process finished with exit code 0